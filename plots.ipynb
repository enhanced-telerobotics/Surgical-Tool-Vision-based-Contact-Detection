{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Accuracy over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "def loss_plot(xlim: (int, int) = None):\n",
    "    # Use glob to find all json files in the specified directory\n",
    "    file_list = glob.glob('results/metrics_*.json')\n",
    "\n",
    "    # Create a color cycle iterator to assign unique colors\n",
    "    color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "\n",
    "    # Plot the data\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Initialize a dictionary to store colors for each label\n",
    "    color_map = {}\n",
    "\n",
    "    # Subplot for Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for file in file_list:\n",
    "        # Extract model_name and label_name from the file name\n",
    "        filename = os.path.basename(file)\n",
    "        model_label = filename.replace('metrics_', '').rsplit('.', 1)[0]\n",
    "\n",
    "        # Check if we have already assigned a color, otherwise get the next color\n",
    "        if model_label not in color_map:\n",
    "            color_map[model_label] = next(color_cycle)\n",
    "\n",
    "        # Read json content\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        epochs = list(range(1, len(data['train_loss']) + 1))\n",
    "        plt.plot(epochs, data['train_loss'], label=f'{model_label}',\n",
    "                 color=color_map[model_label])\n",
    "        if len(data['val_acc']) > 0:\n",
    "            plt.plot(epochs, data['val_loss'], linestyle='--', color=color_map[model_label])\n",
    "\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlim(xlim)\n",
    "\n",
    "    # Subplot for Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for file in file_list:\n",
    "        # Extract model_name and label_name from the file name\n",
    "        filename = os.path.basename(file)\n",
    "        model_label = filename.replace('metrics_', '').rsplit('.', 1)[0]\n",
    "\n",
    "        # Read json content\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        epochs = list(range(1, len(data['train_acc']) + 1))\n",
    "        plt.plot(epochs, data['train_acc'], label=f'{model_label}',\n",
    "                 color=color_map[model_label])\n",
    "        if len(data['val_acc']) > 0:\n",
    "            plt.plot(epochs, data['val_acc'], linestyle='--', color=color_map[model_label])\n",
    "\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    # 添加自定义图例\n",
    "    custom_lines = [plt.Line2D([0], [0], color='black', lw=2),\n",
    "                    plt.Line2D([0], [0], color='black', lw=2, linestyle='--')]\n",
    "    plt.figlegend(custom_lines, ['Train', 'Val'], loc='lower center', ncol=2)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.legend()\n",
    "    plt.xlim(xlim)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "loss_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "file_list = glob.glob('results/metrics_*.json')\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(file)\n",
    "    print(int(np.argmin(data['val_loss'][start_epoch:]))+start_epoch)\n",
    "    print(int(np.argmax(data['val_acc'][start_epoch:]))+start_epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from VisionDatasets import ContactDataset\n",
    "from PIL import Image, ImageDraw\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "paths_json = os.path.join(os.getcwd(), 'settings/paths.json')\n",
    "with open(paths_json, 'r') as json_file:\n",
    "    paths = json.load(json_file)\n",
    "\n",
    "data = pd.read_csv(paths['silicone']['labels'], header=0).sample(n=12)\n",
    "\n",
    "dataset = ContactDataset(\n",
    "    images=data['HPC_Path'].tolist(),\n",
    "    labels=data['GT'].to_numpy(),\n",
    "    coords=list(zip(\n",
    "        data['x'].astype(int),\n",
    "        data['y'].astype(int))),\n",
    "    jitter=False)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=1024,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "width, height = 936, 702\n",
    "merged_image = Image.new('RGB', (width, height))\n",
    "\n",
    "draw = ImageDraw.Draw(merged_image)\n",
    "\n",
    "small_image_width, small_image_height = 234, 234\n",
    "x, y = 0, 0\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    for image in images:\n",
    "        image = torchvision.transforms.ToPILImage()(image)\n",
    "        merged_image.paste(image, (x, y))\n",
    "    \n",
    "        x += small_image_width\n",
    "        \n",
    "        if x + small_image_width > width:\n",
    "            x = 0\n",
    "            y += small_image_height\n",
    "\n",
    "merged_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add coordinates to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_json = os.path.join(os.getcwd(), 'settings/paths.json')\n",
    "with open(paths_json, 'r') as json_file:\n",
    "    paths = json.load(json_file)\n",
    "\n",
    "model_root = f'/home/{os.getlogin()}/ERIE/realistic/Track-Shuyuan-2023-08-13/videos'\n",
    "model_name = 'DLC_resnet50_TrackAug13shuffle1_70000'\n",
    "\n",
    "training_sets = [\"M1_L_V1_1\", \"M1_L_V1_2\", \"M1_R_V1_1\", \"M1_R_V1_2\",\n",
    "                 \"M3_L_V1_1\", \"M3_L_V1_2\", \"M3_R_V1_1\", \"M3_R_V1_2\",\n",
    "                 \"M5_L_V1_1\", \"M5_L_V1_2\", \"M5_R_V1_1\", \"M5_R_V1_2\"]\n",
    "test_sets = [\"M1_L_V2_1\", \"M1_L_V2_2\", \"M1_R_V2_1\", \"M1_R_V2_2\",\n",
    "             \"M3_L_V2_1\", \"M3_L_V2_2\", \"M3_R_V2_1\", \"M3_R_V2_2\",\n",
    "             \"M5_L_V2_1\", \"M5_L_V2_2\", \"M5_R_V2_1\", \"M5_R_V2_2\",\n",
    "             \"M2_L_V1_1\", \"M2_L_V1_2\", \"M2_R_V1_1\", \"M2_R_V1_2\",\n",
    "             \"M4_L_V1_1\", \"M4_L_V1_2\", \"M4_R_V1_1\", \"M4_R_V1_2\",\n",
    "             \"M2_L_V2_1\", \"M2_L_V2_2\", \"M2_R_V2_1\", \"M2_R_V2_2\",\n",
    "             \"M4_L_V2_1\", \"M4_L_V2_2\", \"M4_R_V2_1\", \"M4_R_V2_2\"]\n",
    "\n",
    "data = pd.read_csv(paths['realistic']['labels'], header=0)\n",
    "\n",
    "coordinates = {}\n",
    "\n",
    "for dataset in training_sets + test_sets:\n",
    "    coordinates[dataset] = pd.read_hdf(\n",
    "        os.path.join(model_root, f'{dataset}_L_h264{model_name}.h5')).loc[:, [\n",
    "            ('DLC_resnet50_TrackAug13shuffle1_70000', 'Mid_1', 'x'),\n",
    "            ('DLC_resnet50_TrackAug13shuffle1_70000', 'Mid_1', 'y')]].to_numpy()\n",
    "    \n",
    "def extract_id(img_filename):\n",
    "    return int(img_filename.split('_')[1].split('.')[0])\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    dataset = row['dataset']\n",
    "    img_num = row['image']\n",
    "    id = extract_id(img_num)\n",
    "    \n",
    "    x, y = coordinates[dataset][id]\n",
    "    data.at[index, 'x'] = x\n",
    "    data.at[index, 'y'] = y\n",
    "\n",
    "data.to_csv(paths['realistic']['labels'], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
