{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import utils\n",
    "from VisionModels import CustomCNN, CustomEfficientNetB3\n",
    "from VisionDatasets import ContactDataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "parameters_json = os.path.join(os.getcwd(), 'settings/parameters.json')\n",
    "paths_json = os.path.join(os.getcwd(), 'settings/paths.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "with open(parameters_json, 'r') as json_file:\n",
    "    params = json.load(json_file)\n",
    "\n",
    "with open(paths_json, 'r') as json_file:\n",
    "    paths = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "random_seed = params[\"random_seed\"]\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "train_set = [1, 3, 5, 7, 8, 10, 12, 14, 15, 17, 19, 21, 41, 98, 107, 143]\n",
    "val_set = [2, 4, 9, 11, 16, 18, 42, 116]\n",
    "test_set = [37, 68, 71, 23, 22, 24, 25, 26, 27, 28, 29,\n",
    "            46, 47, 38, 95, 104, 101, 92, 110, 45, 125, 134]\n",
    "\n",
    "mapping_set = {\n",
    "    1: 'C_M1_T1_4', 3: 'C_M1_T1_7', 5: 'C_M1_T1_3', 7: 'C_M1_T1_6', 8: 'R2_M1_T1_6', 10: 'R2_M1_T1_7', 12: 'R2_M1_T1_3',\n",
    "    14: 'R2_M1_T1_5', 15: 'L2_M1_T1_3', 17: 'L2_M1_T1_4', 19: 'L2_M1_T1_5', 21: 'L2_M1_T1_7', 41: 'Z2_M1_T1_1',\n",
    "    98: 'M3_Z2_NF', 107: 'M5_Z2_NF', 143: 'M12_Z2_NF', 2: 'C_M1_T1_1', 4: 'C_M1_T1_5', 9: 'R2_M1_T1_2', 11: 'R2_M1_T1_4',\n",
    "    16: 'L2_M1_T1_6', 18: 'L2_M1_T1_1', 42: 'Z2_M1_T1_2', 116: 'M7_Z2_NF', 37: 'C_M1_T1_8', 68: 'M1_NF', 71: 'M2_NF',\n",
    "    23: 'R1_M1_T1_1', 22: 'R1_M1_T1_2', 24: 'R3_M1_T1_1', 25: 'R3_M1_T1_2', 26: 'L1_M1_T1_1', 27: 'L1_M1_T1_2',\n",
    "    28: 'L3_M1_T1_1', 29: 'L3_M1_T1_2', 46: 'Z1_M1_T1_1', 47: 'Z3_M1_T1_1', 38: 'R2_M1_T1_8', 95: 'M3_R2_NF',\n",
    "    104: 'M5_R2_NF', 101: 'M5_L2_NF', 92: 'M3_L2_NF', 110: 'M7_L2_NF', 45: 'Z2_M1_T1_5', 125: 'M8_Z2_NF', 134: 'M9_Z2_NF'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "data_set = 'silicone'\n",
    "model_set = ['CustomCNN', 'EfficientNet']\n",
    "label_set = ['GT', 'MTurk']\n",
    "model_combinations = list(product(model_set, label_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(paths[data_set]['labels'], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data rows by the index of sets\n",
    "train_data = data[data['HPC_Path'].str.extract(\n",
    "    r'imageset_(\\d+)').astype(int).isin(train_set).any(axis=1)]\n",
    "val_data = data[data['HPC_Path'].str.extract(\n",
    "    r'imageset_(\\d+)').astype(int).isin(val_set).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data to Datasets\n",
    "train_dataloader = {}\n",
    "val_dataloader = {}\n",
    "loss_fn = {}\n",
    "\n",
    "for label_name in label_set:\n",
    "\n",
    "    image_col = 'HPC_Path'\n",
    "    \n",
    "    if label_name == 'GT':\n",
    "        label_col = 'GT'\n",
    "    elif label_name == 'MTurk':\n",
    "        label_col = 'MTurk_Label'\n",
    "\n",
    "    train_dataset = ContactDataset(\n",
    "        images=train_data[image_col].tolist(),\n",
    "        labels=train_data[label_col].to_numpy(),\n",
    "        coords=list(zip(\n",
    "            train_data['x'].astype(int),\n",
    "            train_data['y'].astype(int))),\n",
    "        jitter=True)\n",
    "\n",
    "    val_dataset = ContactDataset(\n",
    "        images=val_data[image_col].tolist(),\n",
    "        labels=val_data[label_col].to_numpy(),\n",
    "        coords=list(zip(\n",
    "            train_data['x'].astype(int),\n",
    "            train_data['y'].astype(int))),\n",
    "        jitter=False)\n",
    "\n",
    "    # create DataLoader with existed Datasets\n",
    "    train_dataloader[label_name] = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "        num_workers=(16 if os.cpu_count() > 16 else os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        shuffle=True)\n",
    "\n",
    "    val_dataloader[label_name] = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "        num_workers=(16 if os.cpu_count() > 16 else os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        shuffle=True)\n",
    "    \n",
    "    weights = train_dataset.getWeights().to(device)\n",
    "    loss_fn[label_name] = nn.CrossEntropyLoss(weight=weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, label_name in model_combinations:\n",
    "    # select the model\n",
    "    if model_name == 'CustomCNN':\n",
    "        model = CustomCNN()\n",
    "    elif model_name == 'EfficientNet':\n",
    "        model = CustomEfficientNetB3()\n",
    "\n",
    "    # set up the optimizer (hyper-parameters)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=params[model_name]['learning_rate'],\n",
    "        weight_decay=params[model_name]['weight_decay'])\n",
    "\n",
    "    # train and retrieve the metrics\n",
    "    utils.train(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn[label_name],\n",
    "        dataloader=train_dataloader[label_name],\n",
    "        val_dataloader=val_dataloader[label_name],\n",
    "        device=device,\n",
    "        use_tqdm=True,\n",
    "        epochs=params['epochs'])\n",
    "\n",
    "    utils.save_metrics(model, model_name, label_name)\n",
    "    utils.save_state_dict(model, model_name, label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "for set_name in tqdm(test_set):\n",
    "    # concat paths\n",
    "    label_path = os.path.join(\n",
    "        paths[data_set]['image_set'], f'labels_{set_name}.txt')\n",
    "    images_path = os.path.join(\n",
    "        paths[data_set]['image_set'], f'imageset_{set_name}')\n",
    "    coordinates_path = os.path.join(\n",
    "        paths[data_set]['keypoints'],\n",
    "        f\"{mapping_set[set_name]}_L_h264{paths[data_set]['keypoints_model']}.h5\")\n",
    "\n",
    "    # load data files\n",
    "    test_data = pd.read_csv(label_path, header=None).iloc[:, 1:4].to_numpy()\n",
    "    coordinates = pd.read_hdf(coordinates_path).loc[:, [\n",
    "        (paths[data_set]['keypoints_model'], 'Mid_1', 'x'),\n",
    "        (paths[data_set]['keypoints_model'], 'Mid_1', 'y')]].to_numpy()\n",
    "\n",
    "    test_images = []\n",
    "    test_laebls = []\n",
    "\n",
    "    # add data to list\n",
    "    force_threshold = 0.2\n",
    "    for index, row in enumerate(test_data):\n",
    "        test_images.append(os.path.join(images_path, f'img_{index}.jpg'))\n",
    "        test_laebls.append(1 if np.sqrt(row.dot(row)) > force_threshold else 0)\n",
    "\n",
    "    # create dataset and dataloader\n",
    "    test_dataset = ContactDataset(\n",
    "        images=test_images,\n",
    "        labels=test_laebls,\n",
    "        coords=coordinates.astype(int).tolist(),\n",
    "        jitter=False)\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=512,\n",
    "        num_workers=(16 if os.cpu_count() > 16 else os.cpu_count()),\n",
    "        pin_memory=True,\n",
    "        shuffle=True)\n",
    "\n",
    "    # predict for each model\n",
    "    for model_name, label_name in model_combinations:\n",
    "        # select the model\n",
    "        if model_name == 'CustomCNN':\n",
    "            model = CustomCNN()\n",
    "        elif model_name == 'EfficientNet':\n",
    "            model = CustomEfficientNetB3()\n",
    "\n",
    "        utils.load_state_dict(model, model_name, label_name)\n",
    "        predictions, ground_truth = utils.predict(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            device=device)\n",
    "\n",
    "        results[(model_name, label_name, set_name)] = {\n",
    "            \"Prediction\": predictions,\n",
    "            \"Ground Truth\": ground_truth\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'labels/{data_set}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pkl', 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'EfficientNet'\n",
    "label_name = 'MTurk'\n",
    "model = CustomEfficientNetB3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.load_state_dict(model, model_name, label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, gt = utils.predict(\n",
    "    model=model,\n",
    "    dataloader=test_dataloader,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for ts in test_set:\n",
    "    pred, gt = results[(model_name, label_name, ts)].values()\n",
    "    binary_predictions = (pred > 0.5).astype(int)\n",
    "    print(accuracy_score(gt, binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the optimizer (hyper-parameters)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=params[model_name]['learning_rate'],\n",
    "    weight_decay=params[model_name]['weight_decay'])\n",
    "\n",
    "# train and retrieve the metrics\n",
    "utils.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn[label_name],\n",
    "    dataloader=train_dataloader[label_name],\n",
    "    val_dataloader=val_dataloader[label_name],\n",
    "    device=device,\n",
    "    use_tqdm=True,\n",
    "    epochs=10)\n",
    "\n",
    "utils.save_metrics(model, model_name, label_name)\n",
    "utils.save_state_dict(model, model_name, label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import torchvision\n",
    "width, height = 936, 702\n",
    "merged_image = Image.new('RGB', (width, height))\n",
    "\n",
    "draw = ImageDraw.Draw(merged_image)\n",
    "\n",
    "small_image_width, small_image_height = 234, 234\n",
    "x, y = 0, 0\n",
    "\n",
    "for images, labels in test_dataloader:\n",
    "    for image in images:\n",
    "        image = torchvision.transforms.ToPILImage()(image)\n",
    "        merged_image.paste(image, (x, y))\n",
    "    \n",
    "        x += small_image_width\n",
    "        \n",
    "        if x + small_image_width > width:\n",
    "            x = 0\n",
    "            y += small_image_height\n",
    "\n",
    "merged_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
